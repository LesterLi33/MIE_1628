{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3192e7-fd68-4bf7-9147-49d21130a9a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType,FloatType\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f3bc78-fcc6-49cb-99ad-028cb84a0be3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of odd number is:  496\nTHe count of even number is :  514\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "spark = SparkSession.builder.appName('counter').getOrCreate()\n",
    "data= spark.read.text(\"dbfs:/FileStore/shared_uploads/leelisooke@gmail.com/integer.txt\")\n",
    "data=data.withColumn(\"value\", col('value').cast(IntegerType()))\n",
    "count_even=data.filter(data.value % 2==0).count()\n",
    "count_odd = data.filter(data.value % 2!=0).count()\n",
    "print(\"The count of odd number is: \",count_odd)\n",
    "print(\"THe count of even number is : \", count_even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ebf2132-422a-4c1e-b7ad-1c28a2f9ea89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|department|sum(salary)|\n+----------+-----------+\n|     Sales|  3488491.0|\n| Developer|  3221394.0|\n|  Research|  3328284.0|\n| Marketing|  3158450.0|\n|        QA|  3360624.0|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Question2 dbfs:/FileStore/shared_uploads/leelisooke@gmail.com/salary.txt\n",
    "spark = SparkSession.builder.appName('sum_of_salary').getOrCreate()\n",
    "data = spark.read.text(\"dbfs:/FileStore/shared_uploads/leelisooke@gmail.com/salary.txt\")\n",
    "data=data.withColumn(\"department\", F.split(F.col('value'),' ')[0])\n",
    "data=data.withColumn(\"salary\", F.split(F.col('value'),' ')[1].cast(FloatType()))\n",
    "data_department_group= data.groupby(\"department\").sum()\n",
    "data_department_group.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436c4ea2-d2e1-4df2-9127-6961d23818b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts:  [('Shakespeare', 22), ('GUTENBERG', 99), ('WILLIAM', 115), ('WORLD', 98), ('COLLEGE', 98), ('When', 393), ('Lord', 341), ('Library', 2)]\n\n\nTop 15 words:  [('', 231583), ('the', 11397), ('and', 8777), ('I', 8556), ('of', 7873), ('to', 7421), ('a', 5672), ('my', 4913), ('in', 4600), ('you', 4060), ('And', 3547), ('that', 3522), ('is', 3481), ('his', 3226), ('with', 3175)]\nbottom 15 words:  [('anyone', 1), ('restrictions', 1), ('whatsoever.', 1), ('re-use', 1), ('online', 1), ('www.gutenberg.org', 1), ('COPYRIGHTED', 1), ('eBook,', 1), ('Details', 1), ('guidelines', 1), ('file.', 1), ('Author:', 1), ('Posting', 1), ('1,', 1), ('2011', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Question 3 dbfs:/FileStore/shared_uploads/leelisooke@gmail.com/shakespeare_1.txt\n",
    "spark = SparkSession.builder.appName('word_count').getOrCreate()\n",
    "data = spark.sparkContext.textFile(\"dbfs:/FileStore/shared_uploads/leelisooke@gmail.com/shakespeare_1.txt\")\n",
    "#mapreduce\n",
    "counts= data.flatMap(lambda line : line.split(\" \"))\\\n",
    "    .map(lambda word: (word,1))\\\n",
    "        .reduceByKey(lambda a, b: a+b)\n",
    "\n",
    "words= [\"Shakespeare\",\"When\",\"Lord\", \"Library\", \"GUTENBERG\",\"WILLIAM\",\"COLLEGE\",\"WORLD\"]\n",
    "\n",
    "words_list= counts.filter(lambda x: x[0] in words)\n",
    "print(\"Word counts: \",words_list.collect())\n",
    "\n",
    "#Question 4:\n",
    "print('\\n')\n",
    "top_words= counts.takeOrdered(15,key=lambda x : -x[1])\n",
    "bot_words= counts.takeOrdered(15, key=lambda x: x[1])\n",
    "\n",
    "print(\"Top 15 words: \",top_words)\n",
    "print(\"Bottom 15 words: \", bot_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0658dab8-ec86-46fa-9d87-11996e9a9fad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n|movieId|       mean_rating|\n+-------+------------------+\n|     32|2.9166666666666665|\n|     90|            2.8125|\n|     30|               2.5|\n|     94| 2.473684210526316|\n|     23| 2.466666666666667|\n|     49|            2.4375|\n|     29|               2.4|\n|     18|               2.4|\n|     52| 2.357142857142857|\n|     53|              2.25|\n|     62|              2.25|\n|     92|2.2142857142857144|\n+-------+------------------+\n\n+------+------------------+\n|userId|       mean_rating|\n+------+------------------+\n|    11|2.2857142857142856|\n|    26| 2.204081632653061|\n|    22|2.1607142857142856|\n|    23|2.1346153846153846|\n|     2|2.0652173913043477|\n|    17|1.9565217391304348|\n|     8|1.8979591836734695|\n|    24|1.8846153846153846|\n|    12|1.8545454545454545|\n|     3|1.8333333333333333|\n|    29| 1.826086956521739|\n|    28|              1.82|\n+------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#part2\n",
    "#Question 1\n",
    "spark = SparkSession.builder.appName('movie').getOrCreate()\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/leelisooke@gmail.com/movies.csv\")\n",
    "top_movie = df.drop('userId')\n",
    "top_movie = top_movie.withColumn('rating', col('rating').cast(IntegerType()))\n",
    "top_movie = df.groupby('movieId').agg(F.mean('rating').alias(\"mean_rating\"))\n",
    "top_movie = top_movie.orderBy(F.col(\"mean_rating\").desc()).limit(12)\n",
    "top_movie.show()\n",
    "\n",
    "top_user = df.drop('movieId')\n",
    "top_user = top_user.withColumn('rating',col('rating').cast(IntegerType()))\n",
    "top_user = top_user.groupby(\"userId\").agg(F.mean('rating').alias(\"mean_rating\"))\n",
    "top_user = top_user.orderBy(F.col('mean_rating').desc()).limit(12)\n",
    "top_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "900b101b-ad46-4a39-b73c-ec1a0e701168",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('model').getOrCreate()\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/leelisooke@gmail.com/movies.csv\")\n",
    "df = df.withColumn('userId',col('userId').cast('int'))\n",
    "df = df.withColumn('movieId',col('movieId').cast('int'))\n",
    "df = df.withColumn('rating',col('rating').cast('int'))\n",
    "training,test= df.randomSplit([0.6,0.4])\n",
    "als = ALS(userCol= 'userId', itemCol='movieId', ratingCol= 'rating', coldStartStrategy='drop')\n",
    "eval = RegressionEvaluator(metricName= 'rmse', labelCol= 'rating',predictionCol= 'prediction')\n",
    "parameters = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 20, 30]) \\\n",
    "    .addGrid(als.maxIter, [20]) \\\n",
    "    .addGrid(als.regParam, [0.1, 0.2]) \\\n",
    "    .addGrid(als.numItemBlocks, [1, 5]) \\\n",
    "    .addGrid(als.numUserBlocks, [1, 5]) \\\n",
    "    .build()\n",
    "trainvs = TrainValidationSplit( estimator=als, estimatorParamMaps=parameters, evaluator=eval)\n",
    "\n",
    "model=trainvs.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae65652b-1537-4472-9904-adf358db5228",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.135104739471443\n"
     ]
    }
   ],
   "source": [
    "prediction= model.transform(test)\n",
    "rmse=eval.evaluate(prediction)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06fff825-f747-469d-95ca-f31856707ae6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9596223361107129\n"
     ]
    }
   ],
   "source": [
    "training_2,test_2= df.randomSplit([0.75,0.25])\n",
    "model_2=trainvs.fit(training_2)\n",
    "prediction_2= model_2.transform(test_2)\n",
    "rmse_2=eval.evaluate(prediction_2)\n",
    "print(rmse_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8de5acac-4ca7-4efe-a144-fa28b7c8fd2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - RMSE: 1.135104739471443, MAE: 0.7754954818894367\nModel 2 - RMSE: 0.9596223361107129, MAE: 0.6580533112224182\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rmse_eva = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse_1 = rmse_eva.evaluate(prediction)\n",
    "rmse_2 = rmse_eva.evaluate(prediction_2)\n",
    "\n",
    "mae_eva = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "mae_1 = mae_eva.evaluate(prediction)\n",
    "mae_2 = mae_eva.evaluate(prediction_2)\n",
    "\n",
    "print(f\"Model 1 - RMSE: {rmse_1}, MAE: {mae_1}\")\n",
    "print(f\"Model 2 - RMSE: {rmse_2}, MAE: {mae_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afb12740-4bb4-44ab-a795-3a3932f69bd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Question 4\n",
    "parameters_2 = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 20, 30]) \\\n",
    "    .addGrid(als.maxIter, [10,20,30]) \\\n",
    "    .addGrid(als.regParam, [0.1, 0.2,0.3]) \\\n",
    "    .addGrid(als.numItemBlocks, [1, 5]) \\\n",
    "    .addGrid(als.numUserBlocks, [1, 5]) \\\n",
    "    .build()\n",
    "trainvs_2 = TrainValidationSplit( estimator=als, estimatorParamMaps=parameters_2, evaluator=eval)\n",
    "model_3=trainvs_2.fit(training_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48641375-df3d-4aa5-bf19-c2121a2258d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse of best model:  0.948573238537854\nBest rank: 10\nBest maxIter: 10\nBest regParam: 0.1\nModel 1 - RMSE: 1.135104739471443, MAE: 0.7754954818894367\nModel 2 - RMSE: 0.9596223361107129, MAE: 0.6580533112224182\nModel 3 - RMSE: 0.948573238537854, MAE: 0.6538727079957155\n"
     ]
    }
   ],
   "source": [
    "best_model = model_3.bestModel\n",
    "prediction_3= best_model.transform(test_2)\n",
    "rmse_3=eval.evaluate(prediction_3)\n",
    "print(\"rmse of best model: \",rmse_3)\n",
    "print(\"Best rank:\", best_model.rank)\n",
    "print(\"Best maxIter:\", best_model._java_obj.parent().getMaxIter())\n",
    "print(\"Best regParam:\", best_model._java_obj.parent().getRegParam())\n",
    "\n",
    "rmse_eva = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse_1 = rmse_eva.evaluate(prediction)\n",
    "rmse_2 = rmse_eva.evaluate(prediction_2)\n",
    "rmse_3 = rmse_eva.evaluate(prediction_3)\n",
    "\n",
    "mae_eva = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "mae_1 = mae_eva.evaluate(prediction)\n",
    "mae_2 = mae_eva.evaluate(prediction_2)\n",
    "mae_3 = mae_eva.evaluate(prediction_3)\n",
    "\n",
    "print(f\"Model 1 - RMSE: {rmse_1}, MAE: {mae_1}\")\n",
    "print(f\"Model 2 - RMSE: {rmse_2}, MAE: {mae_2}\")\n",
    "print(f\"Model 3 - RMSE: {rmse_3}, MAE: {mae_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e79a5c0-5eef-4484-9540-fe3de65eef9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|userId|recommendations                                                                                                                                                                                       |\n+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|12    |[{17, 4.4534}, {48, 4.315681}, {27, 4.1103806}, {32, 3.978695}, {90, 3.8604753}, {35, 3.7725995}, {46, 3.5740008}, {94, 3.5378063}, {18, 3.534183}, {50, 3.5021763}, {16, 3.4162762}, {55, 3.3012254}]|\n|10    |[{92, 3.427152}, {2, 3.161942}, {25, 2.8951662}, {89, 2.8869176}, {40, 2.862807}, {49, 2.7971985}, {12, 2.662564}, {42, 2.5345876}, {62, 2.5064726}, {0, 2.2817194}, {95, 2.1507835}, {9, 2.0935311}] |\n+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\nNone\n"
     ]
    }
   ],
   "source": [
    "#Question 5\n",
    "user_ids = spark.createDataFrame([(10,), (12,)], [\"userId\"])\n",
    "user_rec= best_model.recommendForUserSubset(user_ids,12)\n",
    "print(user_rec.show(truncate=False))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1628_AS2 2024-10-08 21:35:34",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
